{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d0f9fed",
   "metadata": {},
   "source": [
    "## Equity in Responsiveness: Analysis of Service Delays by Neighborhood\n",
    "\n",
    "Are some neighborhoods systematically slower to receive service, even after accounting for service type?\n",
    "We isolate government responsiveness by computing the delay relative to expected resolution time for each service type.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df120445",
   "metadata": {},
   "source": [
    "# Equity Analysis\n",
    "\n",
    "Here we will see whether 311 requests are equitably solved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ed4b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d485d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "import geopandas as gpd\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Create figures directory if it doesn't exist\n",
    "os.makedirs(\"../figures\", exist_ok=True)\n",
    "\n",
    "sf_serv_req = gpd.read_parquet(\"../data/processed/serv_req_cleaned.parquet\")\n",
    "sf_tracts_gdf = gpd.read_file(\"../data/processed/sf_tracts_cleaned_2023.gpkg\")\n",
    "\n",
    "# Filter to closed requests only\n",
    "closed_req = sf_serv_req[sf_serv_req[\"closed\"].notna()].copy()\n",
    "\n",
    "# Compute resolution time in hours\n",
    "closed_req[\"resolution_hours\"] = (\n",
    "    closed_req[\"closed\"] - closed_req[\"opened\"]\n",
    ").dt.total_seconds() / 3600.0\n",
    "closed_req = closed_req[closed_req[\"resolution_hours\"] >= 0]\n",
    "\n",
    "print(f\"Total closed requests: {len(closed_req)}\")\n",
    "print(f\"Resolution time stats (hours):\")\n",
    "print(closed_req[\"resolution_hours\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4cce15",
   "metadata": {},
   "source": [
    "### Step 1: Expected Resolution Time by Service Type\n",
    "\n",
    "Calculate median resolution time for each service type to establish the baseline expectation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26575029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use request_type as the service classification\n",
    "service_col = \"Category\"\n",
    "\n",
    "print(f\"Using service column: {service_col}\")\n",
    "print(f\"Unique service types: {closed_req[service_col].nunique()}\")\n",
    "\n",
    "# Compute expected (median) resolution time by service type\n",
    "expected_resolution = (\n",
    "    closed_req.groupby(service_col)[\"resolution_hours\"].median().reset_index()\n",
    ")\n",
    "expected_resolution.columns = [service_col, \"expected_hours\"]\n",
    "\n",
    "print(f\"\\nExpected resolution time by service type (top 15):\")\n",
    "display(expected_resolution.nlargest(15, \"expected_hours\"))\n",
    "\n",
    "# Store for later use\n",
    "expected_dict = dict(\n",
    "    zip(expected_resolution[service_col], expected_resolution[\"expected_hours\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3432806",
   "metadata": {},
   "source": [
    "### Step 2: Calculate Delay Per Request\n",
    "\n",
    "Compute actual delay as the difference between observed and expected resolution time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f3868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map expected resolution to each request\n",
    "closed_req[\"expected_hours\"] = closed_req[service_col].map(expected_dict)\n",
    "\n",
    "# Drop requests with unmapped service types\n",
    "closed_req_mapped = closed_req.dropna(subset=[\"expected_hours\"]).copy()\n",
    "\n",
    "# Compute delay (positive = slower than expected)\n",
    "closed_req_mapped[\"delay_hours\"] = (\n",
    "    closed_req_mapped[\"resolution_hours\"] - closed_req_mapped[\"expected_hours\"]\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Requests with mapped service types: {len(closed_req_mapped)} / {len(closed_req)}\"\n",
    ")\n",
    "print(f\"\\nDelay statistics (hours):\")\n",
    "print(closed_req_mapped[\"delay_hours\"].describe())\n",
    "print(\n",
    "    f\"\\nRequests faster than expected: {(closed_req_mapped['delay_hours'] < 0).sum()}\"\n",
    ")\n",
    "print(f\"Requests slower than expected: {(closed_req_mapped['delay_hours'] > 0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d020f07d",
   "metadata": {},
   "source": [
    "### Step 3: Spatial Join and Aggregate Delay by Tract\n",
    "\n",
    "Match requests to census tracts and compute average delay.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94159a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure same CRS\n",
    "if closed_req_mapped.crs != sf_tracts_gdf.crs:\n",
    "    closed_req_mapped = closed_req_mapped.to_crs(sf_tracts_gdf.crs)\n",
    "\n",
    "# Spatial join: requests within tracts\n",
    "req_with_tract = gpd.sjoin(\n",
    "    closed_req_mapped,\n",
    "    sf_tracts_gdf[[\"GEOID\", \"geometry\"]],\n",
    "    how=\"left\",\n",
    "    predicate=\"within\",\n",
    ")\n",
    "\n",
    "# Handle GEOID column name (may have suffix from sjoin)\n",
    "geoid_col_sjoin = \"GEOID_right\" if \"GEOID_right\" in req_with_tract.columns else \"GEOID\"\n",
    "\n",
    "print(\n",
    "    f\"Requests matched to tracts: {req_with_tract[geoid_col_sjoin].notna().sum()} / {len(req_with_tract)}\"\n",
    ")\n",
    "\n",
    "# Aggregate delay by tract\n",
    "tract_delay = (\n",
    "    req_with_tract.dropna(subset=[geoid_col_sjoin])\n",
    "    .groupby(geoid_col_sjoin)\n",
    "    .agg(\n",
    "        {\n",
    "            \"delay_hours\": [\"mean\", \"median\", \"std\", \"count\"],\n",
    "            \"resolution_hours\": \"mean\",\n",
    "            \"expected_hours\": \"mean\",\n",
    "        }\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "tract_delay.columns = [\n",
    "    \"GEOID\",\n",
    "    \"avg_delay_hours\",\n",
    "    \"median_delay_hours\",\n",
    "    \"std_delay_hours\",\n",
    "    \"request_count\",\n",
    "    \"avg_resolution_hours\",\n",
    "    \"avg_expected_hours\",\n",
    "]\n",
    "\n",
    "print(f\"\\nTract delay statistics:\")\n",
    "print(\n",
    "    tract_delay[[\"avg_delay_hours\", \"median_delay_hours\", \"request_count\"]].describe()\n",
    ")\n",
    "\n",
    "# Sort by average delay\n",
    "tract_delay_sorted = tract_delay.sort_values(\"avg_delay_hours\", ascending=False)\n",
    "print(f\"\\nTracts with LONGEST delays (top 10):\")\n",
    "display(\n",
    "    tract_delay_sorted.head(10)[\n",
    "        [\"GEOID\", \"avg_delay_hours\", \"median_delay_hours\", \"request_count\"]\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"\\nTracts with SHORTEST delays (bottom 10):\")\n",
    "display(\n",
    "    tract_delay_sorted.tail(10)[\n",
    "        [\"GEOID\", \"avg_delay_hours\", \"median_delay_hours\", \"request_count\"]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d47c7c",
   "metadata": {},
   "source": [
    "### Step 4: Map Delay by Tract\n",
    "\n",
    "Visualize which neighborhoods have longer-than-expected delays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34138667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge delay data with tract geometries\n",
    "tract_delay_map = sf_tracts_gdf.merge(tract_delay, on=\"GEOID\", how=\"left\")\n",
    "\n",
    "# Create choropleth map of delay\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 12))\n",
    "\n",
    "# Use red-to-green colormap: red = positive delay (slower), green = negative delay (faster)\n",
    "# Set vmin/vmax based on actual data to capture all meaningful variation (5th to 95th percentile)\n",
    "vmin = tract_delay[\"avg_delay_hours\"].quantile(0.05)\n",
    "vmax = tract_delay[\"avg_delay_hours\"].quantile(0.95)\n",
    "\n",
    "tract_delay_map.plot(\n",
    "    column=\"avg_delay_hours\",\n",
    "    ax=ax,\n",
    "    cmap=\"RdYlGn_r\",  # Red for positive (slow), Green for negative (fast)\n",
    "    edgecolor=\"gray\",\n",
    "    linewidth=0.5,\n",
    "    legend=True,\n",
    "    vmin=vmin,\n",
    "    vmax=vmax,\n",
    "    legend_kwds={\n",
    "        \"label\": \"Average Delay (hours)\\n(+ = slower than expected)\",\n",
    "        \"orientation\": \"horizontal\",\n",
    "        \"shrink\": 0.6,\n",
    "    },\n",
    ")\n",
    "\n",
    "ax.set_title(\n",
    "    \"Service Request Delay by Census Tract\\n(Actual Resolution - Expected Resolution)\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../figures/delay_choropleth_by_tract.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Red areas = Neighborhoods with longer delays than expected (inequitable)\")\n",
    "\n",
    "print(f\"Green areas = Neighborhoods with shorter delays than expected (advantaged)\")\n",
    "print(f\"\\n✓ Figure saved to ../figures/delay_choropleth_by_tract.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c2ba60",
   "metadata": {},
   "source": [
    "### Step 5: Correlation Analysis - Delay vs. Demographic/Economic Factors\n",
    "\n",
    "Test whether delays correlate with income, housing costs, and racial composition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd02c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare correlation dataframe with relevant demographic variables\n",
    "# List of demographic columns to check (using actual column names)\n",
    "demographic_cols = [\n",
    "    \"median_household_income\",\n",
    "    \"median_home_value\",\n",
    "    \"median_rent\",\n",
    "    \"poverty_rate\",\n",
    "    \"gini_index\",\n",
    "    \"pct_renter_occupied\",\n",
    "    \"share_white\",\n",
    "    \"share_black\",\n",
    "    \"share_asian\",\n",
    "    \"share_hispanic\",\n",
    "    \"bachelors_plus_rate\",\n",
    "    \"total_population\",\n",
    "]\n",
    "\n",
    "# Get available columns\n",
    "available_cols = [col for col in demographic_cols if col in sf_tracts_gdf.columns]\n",
    "\n",
    "print(f\"Available demographic variables: {available_cols}\")\n",
    "\n",
    "# Build correlation dataframe\n",
    "corr_df = sf_tracts_gdf[[\"GEOID\"] + available_cols].merge(\n",
    "    tract_delay[[\"GEOID\", \"avg_delay_hours\", \"median_delay_hours\", \"request_count\"]],\n",
    "    on=\"GEOID\",\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "# Filter to tracts with at least 10 requests for reliability\n",
    "corr_df = corr_df[corr_df[\"request_count\"] >= 10]\n",
    "\n",
    "print(f\"\\nCorrelation analysis based on {len(corr_df)} tracts with 10+ requests\")\n",
    "\n",
    "# Compute Pearson correlations\n",
    "print(\"\\n=== PEARSON CORRELATIONS WITH AVERAGE DELAY ===\\n\")\n",
    "correlations = {}\n",
    "for col in available_cols:\n",
    "    if corr_df[col].notna().sum() > 5:  # Require at least 5 valid pairs\n",
    "        r, p_val = pearsonr(\n",
    "            corr_df[col].dropna(), corr_df.loc[corr_df[col].notna(), \"avg_delay_hours\"]\n",
    "        )\n",
    "        correlations[col] = {\"r\": r, \"p_value\": p_val}\n",
    "        sig = (\n",
    "            \"***\"\n",
    "            if p_val < 0.001\n",
    "            else \"**\" if p_val < 0.01 else \"*\" if p_val < 0.05 else \"ns\"\n",
    "        )\n",
    "        print(f\"{col:30s}: r = {r:7.3f}, p = {p_val:.4f} {sig}\")\n",
    "\n",
    "print(\"\\nSignificance levels: *** p<0.001, ** p<0.01, * p<0.05, ns = not significant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3870c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize key correlations\n",
    "sig_cols = [col for col, stats in correlations.items() if stats[\"p_value\"] < 0.05]\n",
    "\n",
    "if len(sig_cols) > 0:\n",
    "    n_plots = len(sig_cols)\n",
    "    n_cols = 2\n",
    "    n_rows = (n_plots + 1) // 2\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 4 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, col in enumerate(sig_cols):\n",
    "        ax = axes[idx]\n",
    "        r = correlations[col][\"r\"]\n",
    "\n",
    "        # Scatter plot with regression line\n",
    "        corr_df_clean = corr_df[[col, \"avg_delay_hours\"]].dropna()\n",
    "\n",
    "        # Remove outliers from both axes using IQR method for robust visualization\n",
    "        Q1_x = corr_df_clean[col].quantile(0.25)\n",
    "        Q3_x = corr_df_clean[col].quantile(0.75)\n",
    "        IQR_x = Q3_x - Q1_x\n",
    "        mask_x = (corr_df_clean[col] >= Q1_x - 1.5 * IQR_x) & (\n",
    "            corr_df_clean[col] <= Q3_x + 1.5 * IQR_x\n",
    "        )\n",
    "\n",
    "        Q1_y = corr_df_clean[\"avg_delay_hours\"].quantile(0.25)\n",
    "        Q3_y = corr_df_clean[\"avg_delay_hours\"].quantile(0.75)\n",
    "        IQR_y = Q3_y - Q1_y\n",
    "        mask_y = (corr_df_clean[\"avg_delay_hours\"] >= Q1_y - 1.5 * IQR_y) & (\n",
    "            corr_df_clean[\"avg_delay_hours\"] <= Q3_y + 1.5 * IQR_y\n",
    "        )\n",
    "\n",
    "        corr_df_filtered = corr_df_clean[mask_x & mask_y]\n",
    "\n",
    "        ax.scatter(\n",
    "            corr_df_filtered[col], corr_df_filtered[\"avg_delay_hours\"], alpha=0.6, s=60\n",
    "        )\n",
    "\n",
    "        # Add trendline (using filtered data)\n",
    "        z = np.polyfit(corr_df_filtered[col], corr_df_filtered[\"avg_delay_hours\"], 1)\n",
    "        p = np.poly1d(z)\n",
    "        x_line = np.linspace(\n",
    "            corr_df_filtered[col].min(), corr_df_filtered[col].max(), 100\n",
    "        )\n",
    "        ax.plot(x_line, p(x_line), \"r--\", linewidth=2, label=f\"r = {r:.3f}\")\n",
    "\n",
    "        ax.set_xlabel(col, fontsize=11)\n",
    "        ax.set_ylabel(\"Average Delay (hours)\", fontsize=11)\n",
    "        ax.set_title(f\"{col} vs. Delay\", fontsize=12, fontweight=\"bold\")\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for idx in range(len(sig_cols), len(axes)):\n",
    "        axes[idx].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        \"../figures/demographic_delay_correlations.png\", dpi=300, bbox_inches=\"tight\"\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nSignificant correlations found with: {', '.join(sig_cols)}\")\n",
    "    print(f\"✓ Figure saved to ../figures/demographic_delay_correlations.png\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo significant correlations found at p < 0.05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408e816b",
   "metadata": {},
   "source": [
    "## Silent Neighborhoods: Where Problems May Exist But Aren't Reported\n",
    "\n",
    "Question: Where might problems exist but residents aren't reporting them?\n",
    "\n",
    "We identify neighborhoods with high vulnerability indicators (poverty, renter share) but LOW 311 usage—potentially indicating barriers to complaint filing or underreporting of issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9f8134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Compute requests per capita\n",
    "silent_df = sf_tracts_gdf[\n",
    "    [\"GEOID\", \"total_population\", \"poverty_rate\", \"pct_renter_occupied\"]\n",
    "].copy()\n",
    "\n",
    "# Merge with request counts\n",
    "silent_df = silent_df.merge(\n",
    "    tract_delay[[\"GEOID\", \"request_count\"]], on=\"GEOID\", how=\"left\"\n",
    ")\n",
    "\n",
    "# Fill NaN request counts with 0\n",
    "silent_df[\"request_count\"] = silent_df[\"request_count\"].fillna(0)\n",
    "\n",
    "# Compute requests per 1000 residents\n",
    "silent_df[\"requests_per_1000\"] = (\n",
    "    silent_df[\"request_count\"] / silent_df[\"total_population\"]\n",
    ") * 1000\n",
    "\n",
    "print(\"Requests per 1000 residents - statistics:\")\n",
    "print(silent_df[\"requests_per_1000\"].describe())\n",
    "\n",
    "# Step 2: Create vulnerability score (higher = more vulnerable)\n",
    "# Normalize and combine poverty rate and renter share\n",
    "silent_df[\"vulnerability_score\"] = (\n",
    "    (silent_df[\"poverty_rate\"] - silent_df[\"poverty_rate\"].min())\n",
    "    / (silent_df[\"poverty_rate\"].max() - silent_df[\"poverty_rate\"].min())\n",
    "    + (silent_df[\"pct_renter_occupied\"] - silent_df[\"pct_renter_occupied\"].min())\n",
    "    / (silent_df[\"pct_renter_occupied\"].max() - silent_df[\"pct_renter_occupied\"].min())\n",
    ") / 2\n",
    "\n",
    "print(\"\\nVulnerability score - statistics:\")\n",
    "print(silent_df[\"vulnerability_score\"].describe())\n",
    "\n",
    "# Step 3: Identify \"Silent\" neighborhoods\n",
    "# High vulnerability + low request rate (below 25th percentile)\n",
    "req_25th = silent_df[\"requests_per_1000\"].quantile(0.25)\n",
    "vuln_75th = silent_df[\"vulnerability_score\"].quantile(0.75)\n",
    "\n",
    "silent_df[\"is_silent\"] = (silent_df[\"vulnerability_score\"] >= vuln_75th) & (\n",
    "    silent_df[\"requests_per_1000\"] <= req_25th\n",
    ")\n",
    "\n",
    "print(f\"\\n=== SILENT NEIGHBORHOODS ===\")\n",
    "print(\n",
    "    f\"Criteria: Vulnerability ≥ 75th percentile AND Requests per 1000 ≤ 25th percentile\"\n",
    ")\n",
    "print(f\"Vulnerability threshold: {vuln_75th:.3f}\")\n",
    "print(f\"Request rate threshold: {req_25th:.2f} per 1000 residents\")\n",
    "print(f\"\\nTotal silent neighborhoods: {silent_df['is_silent'].sum()}\")\n",
    "\n",
    "silent_neighborhoods = silent_df[silent_df[\"is_silent\"]].sort_values(\n",
    "    \"vulnerability_score\", ascending=False\n",
    ")\n",
    "print(f\"\\nTop Silent Neighborhoods (high vulnerability, low reporting):\")\n",
    "display(\n",
    "    silent_neighborhoods[\n",
    "        [\n",
    "            \"GEOID\",\n",
    "            \"poverty_rate\",\n",
    "            \"pct_renter_occupied\",\n",
    "            \"vulnerability_score\",\n",
    "            \"requests_per_1000\",\n",
    "            \"request_count\",\n",
    "            \"total_population\",\n",
    "        ]\n",
    "    ].head(15)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f8c955",
   "metadata": {},
   "source": [
    "### Visualization: Silent Neighborhoods Map\n",
    "\n",
    "Map showing which tracts are silent (high vulnerability + low reporting).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eee1041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge silent indicators with tract geometries\n",
    "silent_map = sf_tracts_gdf.merge(\n",
    "    silent_df[[\"GEOID\", \"vulnerability_score\", \"requests_per_1000\", \"is_silent\"]],\n",
    "    on=\"GEOID\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# Create two subplots: vulnerability + request rate\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Plot 1: Vulnerability Score\n",
    "silent_map.plot(\n",
    "    column=\"vulnerability_score\",\n",
    "    ax=axes[0],\n",
    "    cmap=\"YlOrRd\",\n",
    "    edgecolor=\"gray\",\n",
    "    linewidth=0.3,\n",
    "    legend=True,\n",
    "    legend_kwds={\n",
    "        \"label\": \"Vulnerability Score\\n(Poverty + Renter Share)\",\n",
    "        \"orientation\": \"vertical\",\n",
    "    },\n",
    ")\n",
    "axes[0].set_title(\"Neighborhood Vulnerability Index\", fontsize=13, fontweight=\"bold\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Plot 2: Request Rate per Capita\n",
    "silent_map.plot(\n",
    "    column=\"requests_per_1000\",\n",
    "    ax=axes[1],\n",
    "    cmap=\"Greens\",\n",
    "    edgecolor=\"gray\",\n",
    "    linewidth=0.3,\n",
    "    legend=True,\n",
    "    legend_kwds={\"label\": \"Requests per 1000 Residents\", \"orientation\": \"vertical\"},\n",
    ")\n",
    "\n",
    "# Highlight silent neighborhoods with red outline\n",
    "silent_only = silent_map[silent_map[\"is_silent\"] == True]\n",
    "if len(silent_only) > 0:\n",
    "    silent_only.plot(\n",
    "        ax=axes[1],\n",
    "        facecolor=\"none\",\n",
    "        edgecolor=\"red\",\n",
    "        linewidth=2,\n",
    "        label=\"Silent Neighborhoods\",\n",
    "    )\n",
    "\n",
    "axes[1].set_title(\"311 Reporting Rate (per Capita)\", fontsize=13, fontweight=\"bold\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../figures/silent_neighborhoods_map.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== INTERPRETATION ===\")\n",
    "print(\"Left map: Darker red = more vulnerable (high poverty + renter share)\")\n",
    "print(\"Right map: Darker green = more 311 requests per capita\")\n",
    "\n",
    "print(\"RED OUTLINES: Silent neighborhoods (vulnerable but low reporting)\")\n",
    "print(f\"\\n✓ Figure saved to ../figures/silent_neighborhoods_map.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8b0bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Vulnerability vs. Request Rate\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Remove outliers using IQR method for requests_per_1000\n",
    "Q1 = silent_df[\"requests_per_1000\"].quantile(0.25)\n",
    "Q3 = silent_df[\"requests_per_1000\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outlier_mask = (silent_df[\"requests_per_1000\"] >= Q1 - 1.5 * IQR) & (\n",
    "    silent_df[\"requests_per_1000\"] <= Q3 + 1.5 * IQR\n",
    ")\n",
    "silent_df_clean = silent_df[outlier_mask]\n",
    "\n",
    "print(\n",
    "    f\"Removed {len(silent_df) - len(silent_df_clean)} outliers (out of {len(silent_df)} tracts)\"\n",
    ")\n",
    "print(\n",
    "    f\"Request rate range after outlier removal: {silent_df_clean['requests_per_1000'].min():.2f} to {silent_df_clean['requests_per_1000'].max():.2f}\"\n",
    ")\n",
    "\n",
    "# Color by silent status\n",
    "colors = silent_df_clean[\"is_silent\"].map({True: \"red\", False: \"steelblue\"})\n",
    "sizes = silent_df_clean[\"total_population\"] / 10  # Scale point size by population\n",
    "\n",
    "ax.scatter(\n",
    "    silent_df_clean[\"vulnerability_score\"],\n",
    "    silent_df_clean[\"requests_per_1000\"],\n",
    "    c=colors,\n",
    "    s=sizes,\n",
    "    alpha=0.6,\n",
    "    edgecolors=\"black\",\n",
    "    linewidth=0.5,\n",
    ")\n",
    "\n",
    "# Add reference lines\n",
    "ax.axvline(\n",
    "    vuln_75th,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    alpha=0.5,\n",
    "    label=f\"High Vulnerability (75th %ile)\",\n",
    ")\n",
    "ax.axhline(\n",
    "    req_25th,\n",
    "    color=\"green\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    alpha=0.5,\n",
    "    label=f\"Low Request Rate (25th %ile)\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Vulnerability Score (normalized)\", fontsize=12)\n",
    "ax.set_ylabel(\"Requests per 1000 Residents\", fontsize=12)\n",
    "ax.set_title(\n",
    "    \"Silent Neighborhoods: High Vulnerability + Low Reporting\\n(Outliers Removed)\",\n",
    "    fontsize=13,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "legend_elements = [\n",
    "    Patch(facecolor=\"red\", alpha=0.6, edgecolor=\"black\", label=\"Silent Neighborhoods\"),\n",
    "    Patch(\n",
    "        facecolor=\"steelblue\", alpha=0.6, edgecolor=\"black\", label=\"Other Neighborhoods\"\n",
    "    ),\n",
    "]\n",
    "ax.legend(handles=legend_elements, fontsize=11, loc=\"upper right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../figures/silent_neighborhoods_scatter.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPoint size represents total population.\")\n",
    "\n",
    "print(\"RED points = Silent neighborhoods (intervention priority)\")\n",
    "print(f\"\\n✓ Figure saved to ../figures/silent_neighborhoods_scatter.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a982270",
   "metadata": {},
   "source": [
    "### Silent Tracts Demographic Comparison\n",
    "\n",
    "How do tracts with the lowest 311 reporting rates compare demographically to those with higher reporting?\n",
    "\n",
    "We compare tracts in the bottom quartile of requests per capita to all others on key ACS indicators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffd3d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define silent tracts (bottom quartile of requests_per_capita)\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Use requests_per_1000 as the reporting metric\n",
    "requests_per_capita_q25 = silent_df[\"requests_per_1000\"].quantile(0.25)\n",
    "\n",
    "silent_df[\"is_silent_quartile\"] = (\n",
    "    silent_df[\"requests_per_1000\"] <= requests_per_capita_q25\n",
    ")\n",
    "\n",
    "n_silent = silent_df[\"is_silent_quartile\"].sum()\n",
    "n_not_silent = (~silent_df[\"is_silent_quartile\"]).sum()\n",
    "\n",
    "print(f\"=== SILENT TRACTS DEFINITION ===\")\n",
    "print(f\"Silent tracts (bottom quartile): {n_silent}\")\n",
    "print(f\"Non-silent tracts: {n_not_silent}\")\n",
    "print(\n",
    "    f\"Reporting rate threshold: {requests_per_capita_q25:.2f} requests per 1000 residents\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b310b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prepare comparison data with ACS characteristics\n",
    "# Check for median_household_income; if not in silent_df, merge from sf_tracts_gdf\n",
    "if \"median_household_income\" not in silent_df.columns:\n",
    "    silent_df = silent_df.merge(\n",
    "        sf_tracts_gdf[[\"GEOID\", \"median_household_income\"]], on=\"GEOID\", how=\"left\"\n",
    "    )\n",
    "\n",
    "# Select demographic columns for comparison\n",
    "comparison_cols = [\n",
    "    \"median_household_income\",\n",
    "    \"pct_renter_occupied\",\n",
    "    \"poverty_rate\",\n",
    "    \"requests_per_1000\",\n",
    "]\n",
    "\n",
    "# Ensure all columns exist\n",
    "available_comparison_cols = [col for col in comparison_cols if col in silent_df.columns]\n",
    "\n",
    "print(f\"\\nAvailable columns for comparison: {available_comparison_cols}\")\n",
    "\n",
    "# Create comparison groups\n",
    "silent_tracts = silent_df[silent_df[\"is_silent_quartile\"]]\n",
    "non_silent_tracts = silent_df[~silent_df[\"is_silent_quartile\"]]\n",
    "\n",
    "print(\n",
    "    f\"\\nSilent tracts n={len(silent_tracts)}, Non-silent tracts n={len(non_silent_tracts)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9766537e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Compute summary statistics and mean differences\n",
    "comparison_results = []\n",
    "\n",
    "for col in available_comparison_cols:\n",
    "    # Get clean data (drop NaNs)\n",
    "    silent_data = silent_tracts[col].dropna()\n",
    "    non_silent_data = non_silent_tracts[col].dropna()\n",
    "\n",
    "    # Compute means\n",
    "    mean_silent = silent_data.mean()\n",
    "    mean_non_silent = non_silent_data.mean()\n",
    "\n",
    "    # Compute difference\n",
    "    difference = mean_silent - mean_non_silent\n",
    "    pct_difference = (\n",
    "        (difference / mean_non_silent * 100) if mean_non_silent != 0 else np.nan\n",
    "    )\n",
    "\n",
    "    # Run t-test if column is income or renter share (as requested)\n",
    "    if col in [\"median_household_income\", \"pct_renter_occupied\"]:\n",
    "        t_stat, p_value = ttest_ind(silent_data, non_silent_data)\n",
    "    else:\n",
    "        t_stat, p_value = np.nan, np.nan\n",
    "\n",
    "    comparison_results.append(\n",
    "        {\n",
    "            \"Characteristic\": col,\n",
    "            \"Silent Tracts (Mean)\": mean_silent,\n",
    "            \"Non-Silent Tracts (Mean)\": mean_non_silent,\n",
    "            \"Difference\": difference,\n",
    "            \"% Difference\": pct_difference,\n",
    "            \"t-statistic\": t_stat,\n",
    "            \"p-value\": p_value,\n",
    "            \"n_silent\": len(silent_data),\n",
    "            \"n_non_silent\": len(non_silent_data),\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Create summary table\n",
    "summary_table = pd.DataFrame(comparison_results)\n",
    "\n",
    "print(\"\\n=== DEMOGRAPHIC COMPARISON: SILENT VS. NON-SILENT TRACTS ===\\n\")\n",
    "display(summary_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3cabd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Detailed interpretation of t-test results\n",
    "print(\"\\n=== T-TEST RESULTS FOR KEY DEMOGRAPHIC DIFFERENCES ===\\n\")\n",
    "\n",
    "for idx, row in summary_table.iterrows():\n",
    "    if row[\"Characteristic\"] in [\"median_household_income\", \"pct_renter_occupied\"]:\n",
    "        print(f\"{row['Characteristic']}:\")\n",
    "\n",
    "        if row[\"Characteristic\"] == \"median_household_income\":\n",
    "            print(f\"  Silent tracts mean: ${row['Silent Tracts (Mean)']:,.0f}\")\n",
    "            print(f\"  Non-silent tracts mean: ${row['Non-Silent Tracts (Mean)']:,.0f}\")\n",
    "            print(f\"  Difference: ${row['Difference']:,.0f}\")\n",
    "        else:  # pct_renter_occupied\n",
    "            print(f\"  Silent tracts mean: {row['Silent Tracts (Mean)']*100:.1f}%\")\n",
    "            print(\n",
    "                f\"  Non-silent tracts mean: {row['Non-Silent Tracts (Mean)']*100:.1f}%\"\n",
    "            )\n",
    "            print(f\"  Difference: {row['Difference']*100:.1f} percentage points\")\n",
    "\n",
    "        print(f\"  % Difference: {row['% Difference']:.1f}%\")\n",
    "\n",
    "        # Interpret p-value\n",
    "        if row[\"p-value\"] < 0.001:\n",
    "            sig_level = \"*** (p < 0.001 - highly significant)\"\n",
    "        elif row[\"p-value\"] < 0.01:\n",
    "            sig_level = \"** (p < 0.01 - very significant)\"\n",
    "        elif row[\"p-value\"] < 0.05:\n",
    "            sig_level = \"* (p < 0.05 - significant)\"\n",
    "        else:\n",
    "            sig_level = \"ns (not significant at p < 0.05)\"\n",
    "\n",
    "        print(\n",
    "            f\"  t-test: t = {row['t-statistic']:.3f}, p = {row['p-value']:.4f} {sig_level}\"\n",
    "        )\n",
    "        print()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Silent tracts have HIGHER median household income (but not significantly)\")\n",
    "print(\"- Silent tracts have SIGNIFICANTLY LOWER renter occupancy (35% lower)\")\n",
    "print(\n",
    "    \"- This suggests silent tracts are more owner-occupied, higher-income neighborhoods\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf664485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Compare demographics between silent and non-silent tracts\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle(\n",
    "    \"Silent Tracts vs. Non-Silent Tracts: Demographic Comparison\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "\n",
    "# Plot 1: Median Household Income\n",
    "ax = axes[0, 0]\n",
    "data_income = [\n",
    "    silent_tracts[\"median_household_income\"].dropna(),\n",
    "    non_silent_tracts[\"median_household_income\"].dropna(),\n",
    "]\n",
    "bp = ax.boxplot(\n",
    "    data_income, labels=[\"Silent\\nTracts\", \"Non-Silent\\nTracts\"], patch_artist=True\n",
    ")\n",
    "for patch, color in zip(bp[\"boxes\"], [\"lightcoral\", \"lightblue\"]):\n",
    "    patch.set_facecolor(color)\n",
    "ax.set_ylabel(\"Median Household Income ($)\", fontsize=11)\n",
    "ax.set_title(\"Income Distribution\", fontsize=12, fontweight=\"bold\")\n",
    "ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f\"${x/1000:.0f}K\"))\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# Plot 2: Renter Occupancy\n",
    "ax = axes[0, 1]\n",
    "data_renter = [\n",
    "    silent_tracts[\"pct_renter_occupied\"].dropna() * 100,\n",
    "    non_silent_tracts[\"pct_renter_occupied\"].dropna() * 100,\n",
    "]\n",
    "bp = ax.boxplot(\n",
    "    data_renter, labels=[\"Silent\\nTracts\", \"Non-Silent\\nTracts\"], patch_artist=True\n",
    ")\n",
    "for patch, color in zip(bp[\"boxes\"], [\"lightcoral\", \"lightblue\"]):\n",
    "    patch.set_facecolor(color)\n",
    "ax.set_ylabel(\"Renter Occupied (%)\", fontsize=11)\n",
    "ax.set_title(\"Renter Occupancy Distribution\", fontsize=12, fontweight=\"bold\")\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# Plot 3: Poverty Rate\n",
    "ax = axes[1, 0]\n",
    "data_poverty = [\n",
    "    silent_tracts[\"poverty_rate\"].dropna() * 100,\n",
    "    non_silent_tracts[\"poverty_rate\"].dropna() * 100,\n",
    "]\n",
    "bp = ax.boxplot(\n",
    "    data_poverty, labels=[\"Silent\\nTracts\", \"Non-Silent\\nTracts\"], patch_artist=True\n",
    ")\n",
    "for patch, color in zip(bp[\"boxes\"], [\"lightcoral\", \"lightblue\"]):\n",
    "    patch.set_facecolor(color)\n",
    "ax.set_ylabel(\"Poverty Rate (%)\", fontsize=11)\n",
    "ax.set_title(\"Poverty Rate Distribution\", fontsize=12, fontweight=\"bold\")\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# Plot 4: Reporting Rate\n",
    "ax = axes[1, 1]\n",
    "data_requests = [\n",
    "    silent_tracts[\"requests_per_1000\"].dropna(),\n",
    "    non_silent_tracts[\"requests_per_1000\"].dropna(),\n",
    "]\n",
    "bp = ax.boxplot(\n",
    "    data_requests, labels=[\"Silent\\nTracts\", \"Non-Silent\\nTracts\"], patch_artist=True\n",
    ")\n",
    "for patch, color in zip(bp[\"boxes\"], [\"lightcoral\", \"lightblue\"]):\n",
    "    patch.set_facecolor(color)\n",
    "ax.set_ylabel(\"Requests per 1000 Residents\", fontsize=11)\n",
    "ax.set_title(\"311 Reporting Rate Distribution\", fontsize=12, fontweight=\"bold\")\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    \"../figures/silent_tracts_demographic_comparison.png\", dpi=300, bbox_inches=\"tight\"\n",
    ")\n",
    "plt.show()\n",
    "print(\"\\n✓ Figure saved to ../figures/silent_tracts_demographic_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca75940",
   "metadata": {},
   "source": [
    "## Service-Level Equity Analysis: Delay Correlations with Demographics\n",
    "\n",
    "For which services does government responsiveness vary most by neighborhood income and housing tenure?\n",
    "\n",
    "We analyze each service category separately to identify which service types show the strongest correlation between delays and demographic factors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46eab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare data with service_base, delay, GEOID, and demographics\n",
    "# Assume closed_req_mapped has service_base, delay_hours, and demographics are available\n",
    "# If using service_col = \"request_type\", adjust to \"service_base\" if available\n",
    "\n",
    "# Check if service_base column exists\n",
    "service_analysis_col = (\n",
    "    \"service_base\" if \"service_base\" in closed_req_mapped.columns else service_col\n",
    ")\n",
    "\n",
    "print(f\"Using service column: {service_analysis_col}\")\n",
    "\n",
    "# Create working dataframe with necessary columns\n",
    "required_cols = [\n",
    "    service_analysis_col,\n",
    "    \"delay_hours\",\n",
    "    \"GEOID\",\n",
    "    \"median_household_income\",\n",
    "    \"pct_renter_occupied\",\n",
    "]\n",
    "\n",
    "# First, merge tract demographics with request data\n",
    "req_with_demographics = req_with_tract[\n",
    "    [service_analysis_col, \"delay_hours\", geoid_col_sjoin]\n",
    "].copy()\n",
    "req_with_demographics = req_with_demographics.rename(columns={geoid_col_sjoin: \"GEOID\"})\n",
    "\n",
    "# Add demographic variables from sf_tracts_gdf\n",
    "demo_vars = req_with_demographics.merge(\n",
    "    sf_tracts_gdf[[\"GEOID\", \"median_household_income\", \"pct_renter_occupied\"]],\n",
    "    on=\"GEOID\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# Drop rows missing any required values\n",
    "demo_vars_clean = demo_vars.dropna(\n",
    "    subset=[\n",
    "        service_analysis_col,\n",
    "        \"delay_hours\",\n",
    "        \"median_household_income\",\n",
    "        \"pct_renter_occupied\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"\\nRequests with complete data: {len(demo_vars_clean)}\")\n",
    "print(f\"Unique services: {demo_vars_clean[service_analysis_col].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a97ea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Filter services with at least 100 resolved requests citywide\n",
    "service_counts = demo_vars_clean[service_analysis_col].value_counts()\n",
    "services_min_100 = service_counts[service_counts >= 100].index.tolist()\n",
    "\n",
    "demo_vars_filtered = demo_vars_clean[\n",
    "    demo_vars_clean[service_analysis_col].isin(services_min_100)\n",
    "].copy()\n",
    "\n",
    "print(f\"Services with ≥100 requests: {len(services_min_100)}\")\n",
    "print(f\"Total requests for these services: {len(demo_vars_filtered)}\")\n",
    "print(f\"\\nService request counts (top 10):\")\n",
    "print(demo_vars_filtered[service_analysis_col].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfebfb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: For each service, compute correlations with delay_hours\n",
    "# Compute Pearson correlation between delay_hours and each demographic variable\n",
    "\n",
    "service_correlations = []\n",
    "\n",
    "for service in services_min_100:\n",
    "    service_data = demo_vars_filtered[\n",
    "        demo_vars_filtered[service_analysis_col] == service\n",
    "    ].copy()\n",
    "\n",
    "    n_requests = len(service_data)\n",
    "\n",
    "    # Correlation with median_household_income\n",
    "    income_delay = service_data[[\"median_household_income\", \"delay_hours\"]].dropna()\n",
    "    if len(income_delay) >= 5:  # Require minimum sample size\n",
    "        r_income, p_income = pearsonr(\n",
    "            income_delay[\"median_household_income\"], income_delay[\"delay_hours\"]\n",
    "        )\n",
    "    else:\n",
    "        r_income, p_income = np.nan, np.nan\n",
    "\n",
    "    # Correlation with percent_renter_occupied\n",
    "    renter_delay = service_data[[\"pct_renter_occupied\", \"delay_hours\"]].dropna()\n",
    "    if len(renter_delay) >= 5:\n",
    "        r_renter, p_renter = pearsonr(\n",
    "            renter_delay[\"pct_renter_occupied\"], renter_delay[\"delay_hours\"]\n",
    "        )\n",
    "    else:\n",
    "        r_renter, p_renter = np.nan, np.nan\n",
    "\n",
    "    service_correlations.append(\n",
    "        {\n",
    "            \"service\": service,\n",
    "            \"n_requests\": n_requests,\n",
    "            \"r_income_delay\": r_income,\n",
    "            \"p_income_delay\": p_income,\n",
    "            \"r_renter_delay\": r_renter,\n",
    "            \"p_renter_delay\": p_renter,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Convert to tidy DataFrame\n",
    "service_corr_df = pd.DataFrame(service_correlations).sort_values(\n",
    "    \"r_income_delay\", ascending=False\n",
    ")\n",
    "\n",
    "print(\"=== SERVICE-LEVEL DELAY CORRELATIONS WITH DEMOGRAPHICS ===\\n\")\n",
    "display(service_corr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a85964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretation: Highlight significant correlations\n",
    "print(\"\\n=== INTERPRETATION ===\\n\")\n",
    "\n",
    "print(\"Services with STRONGEST POSITIVE income–delay correlation\")\n",
    "print(\"(higher income → longer delays):\\n\")\n",
    "positive_income = service_corr_df[service_corr_df[\"r_income_delay\"] > 0.2].head(5)\n",
    "for idx, row in positive_income.iterrows():\n",
    "    sig = (\n",
    "        \"***\"\n",
    "        if row[\"p_income_delay\"] < 0.001\n",
    "        else (\n",
    "            \"**\"\n",
    "            if row[\"p_income_delay\"] < 0.01\n",
    "            else \"*\" if row[\"p_income_delay\"] < 0.05 else \"ns\"\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        f\"  {row['service']:40s}: r = {row['r_income_delay']:6.3f} ({sig}), n = {row['n_requests']}\"\n",
    "    )\n",
    "\n",
    "print(\"\\n\\nServices with STRONGEST NEGATIVE income–delay correlation\")\n",
    "print(\"(higher income → shorter delays):\\n\")\n",
    "negative_income = service_corr_df[service_corr_df[\"r_income_delay\"] < -0.2].head(5)\n",
    "for idx, row in negative_income.iterrows():\n",
    "    sig = (\n",
    "        \"***\"\n",
    "        if row[\"p_income_delay\"] < 0.001\n",
    "        else (\n",
    "            \"**\"\n",
    "            if row[\"p_income_delay\"] < 0.01\n",
    "            else \"*\" if row[\"p_income_delay\"] < 0.05 else \"ns\"\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        f\"  {row['service']:40s}: r = {row['r_income_delay']:6.3f} ({sig}), n = {row['n_requests']}\"\n",
    "    )\n",
    "\n",
    "print(\"\\n\\nServices with STRONGEST renter–delay correlation\")\n",
    "print(\"(higher renter share → longer delays):\\n\")\n",
    "renter_corr = service_corr_df.sort_values(\"r_renter_delay\", ascending=False).head(5)\n",
    "for idx, row in renter_corr.iterrows():\n",
    "    sig = (\n",
    "        \"***\"\n",
    "        if row[\"p_renter_delay\"] < 0.001\n",
    "        else (\n",
    "            \"**\"\n",
    "            if row[\"p_renter_delay\"] < 0.01\n",
    "            else \"*\" if row[\"p_renter_delay\"] < 0.05 else \"ns\"\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        f\"  {row['service']:40s}: r = {row['r_renter_delay']:6.3f} ({sig}), n = {row['n_requests']}\"\n",
    "    )\n",
    "\n",
    "print(\"\\n\\nSignificance levels: *** p<0.001, ** p<0.01, * p<0.05, ns = not significant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d1a1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Bar plot of income–delay correlations sorted by strength\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 10))\n",
    "\n",
    "# Plot 1: Income-Delay Correlations\n",
    "service_corr_sorted = service_corr_df.sort_values(\"r_income_delay\")\n",
    "colors_income = [\n",
    "    \"red\" if x < -0.1 else \"orange\" if x < 0 else \"lightblue\" if x < 0.1 else \"blue\"\n",
    "    for x in service_corr_sorted[\"r_income_delay\"]\n",
    "]\n",
    "\n",
    "axes[0].barh(\n",
    "    range(len(service_corr_sorted)),\n",
    "    service_corr_sorted[\"r_income_delay\"],\n",
    "    color=colors_income,\n",
    ")\n",
    "axes[0].set_yticks(range(len(service_corr_sorted)))\n",
    "axes[0].set_yticklabels(service_corr_sorted[\"service\"], fontsize=8)\n",
    "axes[0].set_xlabel(\"Correlation (delay hours vs. median income)\", fontsize=11)\n",
    "axes[0].set_title(\n",
    "    \"Service-Level Income–Delay Correlation\\n(Red = inequitable: higher income → longer delays)\",\n",
    "    fontsize=12,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "axes[0].axvline(0, color=\"black\", linestyle=\"-\", linewidth=0.8)\n",
    "axes[0].grid(True, alpha=0.3, axis=\"x\")\n",
    "\n",
    "# Plot 2: Renter–Delay Correlations\n",
    "service_corr_renter = service_corr_df.sort_values(\"r_renter_delay\")\n",
    "colors_renter = [\n",
    "    \"red\" if x < -0.1 else \"orange\" if x < 0 else \"lightblue\" if x < 0.1 else \"blue\"\n",
    "    for x in service_corr_renter[\"r_renter_delay\"]\n",
    "]\n",
    "\n",
    "axes[1].barh(\n",
    "    range(len(service_corr_renter)),\n",
    "    service_corr_renter[\"r_renter_delay\"],\n",
    "    color=colors_renter,\n",
    ")\n",
    "axes[1].set_yticks(range(len(service_corr_renter)))\n",
    "axes[1].set_yticklabels(service_corr_renter[\"service\"], fontsize=8)\n",
    "axes[1].set_xlabel(\"Correlation (delay hours vs. renter share)\", fontsize=11)\n",
    "axes[1].set_title(\n",
    "    \"Service-Level Renter–Delay Correlation\\n(Red = inequitable: higher renter share → longer delays)\",\n",
    "    fontsize=12,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "axes[1].axvline(0, color=\"black\", linestyle=\"-\", linewidth=0.8)\n",
    "axes[1].grid(True, alpha=0.3, axis=\"x\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../figures/service_level_correlations.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    \"\\nColor coding: Red = negative correlation (inequitable), Blue = positive correlation\"\n",
    ")\n",
    "print(f\"✓ Figure saved to ../figures/service_level_correlations.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd32cb80",
   "metadata": {},
   "source": [
    "## Counterfactual / Residual Equity Analysis\n",
    "\n",
    "**Question**: After controlling for service type, request volume, and demographic factors, which neighborhoods still experience systematically longer delays?\n",
    "\n",
    "We fit a regression model predicting delay_hours using:\n",
    "\n",
    "- Service category (fixed effects)\n",
    "- Median household income\n",
    "- Percent renter-occupied\n",
    "- Request volume per tract\n",
    "\n",
    "Then identify tracts with the largest **positive residuals** (actual delays exceed what the model predicts).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df66d4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare request-level DataFrame with all required variables\n",
    "# We need: delay_hours, service_base, GEOID, median_household_income, pct_renter_occupied, request_volume_tract\n",
    "\n",
    "# Start with demo_vars_clean which already has delay_hours, service, GEOID, and demographics\n",
    "residual_df = demo_vars_clean.copy()\n",
    "\n",
    "# Use service_analysis_col as service_base\n",
    "residual_df = residual_df.rename(columns={service_analysis_col: \"service_base\"})\n",
    "\n",
    "# Compute request volume per tract\n",
    "request_volume_by_tract = (\n",
    "    residual_df.groupby(\"GEOID\").size().reset_index(name=\"request_volume_tract\")\n",
    ")\n",
    "\n",
    "# Merge request volume back to request-level data\n",
    "residual_df = residual_df.merge(request_volume_by_tract, on=\"GEOID\", how=\"left\")\n",
    "\n",
    "print(\"=== STEP 1: REQUEST-LEVEL DATA PREPARATION ===\")\n",
    "print(f\"Total requests with complete data: {len(residual_df)}\")\n",
    "print(f\"Unique tracts: {residual_df['GEOID'].nunique()}\")\n",
    "print(f\"Unique service types: {residual_df['service_base'].nunique()}\")\n",
    "print(\"\\nColumns available:\")\n",
    "print(\n",
    "    residual_df[\n",
    "        [\n",
    "            \"delay_hours\",\n",
    "            \"service_base\",\n",
    "            \"GEOID\",\n",
    "            \"median_household_income\",\n",
    "            \"pct_renter_occupied\",\n",
    "            \"request_volume_tract\",\n",
    "        ]\n",
    "    ].head()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5746955e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Fit linear regression model using statsmodels\n",
    "# Predict delay_hours using service_base (categorical), income, renter_pct, and request_volume\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Prepare data: ensure no missing values in predictors\n",
    "model_data = residual_df[\n",
    "    [\n",
    "        \"delay_hours\",\n",
    "        \"service_base\",\n",
    "        \"median_household_income\",\n",
    "        \"pct_renter_occupied\",\n",
    "        \"request_volume_tract\",\n",
    "        \"GEOID\",\n",
    "    ]\n",
    "].dropna()\n",
    "\n",
    "print(\"=== STEP 2: FIT LINEAR REGRESSION MODEL ===\")\n",
    "print(f\"Data for modeling: {len(model_data)} requests\")\n",
    "print(f\"\\nModel specification:\")\n",
    "print(\n",
    "    \"  delay_hours ~ C(service_base) + median_household_income + pct_renter_occupied + request_volume_tract\"\n",
    ")\n",
    "print(\"\\nC(service_base) creates categorical fixed effects for each service type\")\n",
    "\n",
    "# Fit OLS regression with service_base as categorical predictor\n",
    "# C() treats service_base as categorical and creates dummy variables\n",
    "model = ols(\n",
    "    \"delay_hours ~ C(service_base) + median_household_income + pct_renter_occupied + request_volume_tract\",\n",
    "    data=model_data,\n",
    ").fit()\n",
    "\n",
    "print(\"\\n=== MODEL SUMMARY ===\")\n",
    "print(model.summary())\n",
    "\n",
    "print(f\"\\nModel R-squared: {model.rsquared:.4f}\")\n",
    "print(f\"Model Adjusted R-squared: {model.rsquared_adj:.4f}\")\n",
    "print(f\"Number of observations: {model.nobs:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac42810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Compute residuals for each request\n",
    "# Residual = Actual delay - Predicted delay\n",
    "# Positive residuals = slower than model predicts (after controlling for all factors)\n",
    "\n",
    "model_data[\"predicted_delay\"] = model.fittedvalues\n",
    "model_data[\"residual\"] = model.resid\n",
    "\n",
    "print(\"=== STEP 3: COMPUTE RESIDUALS ===\")\n",
    "print(f\"Residuals computed for {len(model_data)} requests\")\n",
    "print(\"\\nResidual statistics:\")\n",
    "print(model_data[\"residual\"].describe())\n",
    "\n",
    "print(\"\\n=== INTERPRETATION ===\")\n",
    "print(\"Positive residual: Actual delay LONGER than model predicts (inequitable)\")\n",
    "print(\"Negative residual: Actual delay SHORTER than model predicts (advantaged)\")\n",
    "print(\"\\nExamples:\")\n",
    "display(\n",
    "    model_data[\n",
    "        [\"GEOID\", \"service_base\", \"delay_hours\", \"predicted_delay\", \"residual\"]\n",
    "    ].head(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c620c902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Aggregate residuals to tract level\n",
    "# Compute mean, median, and count of residuals per tract\n",
    "\n",
    "tract_residuals = (\n",
    "    model_data.groupby(\"GEOID\")\n",
    "    .agg({\"residual\": [\"mean\", \"median\", \"count\"]})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Flatten column names\n",
    "tract_residuals.columns = [\"GEOID\", \"mean_residual\", \"median_residual\", \"n_requests\"]\n",
    "\n",
    "print(\"=== STEP 4: AGGREGATE RESIDUALS TO TRACT LEVEL ===\")\n",
    "print(f\"Total tracts: {len(tract_residuals)}\")\n",
    "print(\"\\nTract-level residual statistics:\")\n",
    "print(tract_residuals[[\"mean_residual\", \"median_residual\", \"n_requests\"]].describe())\n",
    "\n",
    "# Add demographic info for context\n",
    "tract_residuals = tract_residuals.merge(\n",
    "    sf_tracts_gdf[\n",
    "        [\"GEOID\", \"median_household_income\", \"pct_renter_occupied\", \"poverty_rate\"]\n",
    "    ],\n",
    "    on=\"GEOID\",\n",
    "    how=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607283c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Filter to tracts with at least 30 requests and identify top 15 worst performers\n",
    "# These are tracts with the highest mean residuals (systematically slower than expected)\n",
    "\n",
    "min_requests = 30\n",
    "tract_residuals_filtered = tract_residuals[\n",
    "    tract_residuals[\"n_requests\"] >= min_requests\n",
    "].copy()\n",
    "\n",
    "print(f\"=== STEP 5: FILTER & IDENTIFY TOP UNDERPERFORMING TRACTS ===\")\n",
    "print(\n",
    "    f\"Tracts with at least {min_requests} requests: {len(tract_residuals_filtered)} / {len(tract_residuals)}\"\n",
    ")\n",
    "\n",
    "# Sort by mean residual (highest = worst)\n",
    "tract_residuals_sorted = tract_residuals_filtered.sort_values(\n",
    "    \"mean_residual\", ascending=False\n",
    ")\n",
    "\n",
    "# Get top 15\n",
    "top_15_worst = tract_residuals_sorted.head(15)\n",
    "\n",
    "print(f\"\\n=== TOP 15 TRACTS WITH HIGHEST UNEXPLAINED DELAYS ===\")\n",
    "print(\n",
    "    \"(Systematically slower than model predicts, even after controlling for service type, income, renter share, and volume)\"\n",
    ")\n",
    "print()\n",
    "display(\n",
    "    top_15_worst[\n",
    "        [\n",
    "            \"GEOID\",\n",
    "            \"mean_residual\",\n",
    "            \"median_residual\",\n",
    "            \"n_requests\",\n",
    "            \"median_household_income\",\n",
    "            \"pct_renter_occupied\",\n",
    "            \"poverty_rate\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\n=== KEY INSIGHTS ===\")\n",
    "print(\n",
    "    f\"Mean residual range for top 15: {top_15_worst['mean_residual'].min():.2f} to {top_15_worst['mean_residual'].max():.2f} hours\"\n",
    ")\n",
    "print(\n",
    "    f\"Average income of top 15: ${top_15_worst['median_household_income'].mean():,.0f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Average renter share of top 15: {top_15_worst['pct_renter_occupied'].mean()*100:.1f}%\"\n",
    ")\n",
    "print(f\"Average poverty rate of top 15: {top_15_worst['poverty_rate'].mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484c5ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Choropleth map of mean residual delay by census tract\n",
    "# Using diverging color scale centered at 0, clipped at 5th and 95th percentiles\n",
    "\n",
    "# Merge residuals with tract geometries\n",
    "residual_map = sf_tracts_gdf.merge(tract_residuals_filtered, on=\"GEOID\", how=\"left\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 12))\n",
    "\n",
    "# Clip extreme values at 5th and 95th percentiles for better visualization\n",
    "vmin_res = tract_residuals_filtered[\"mean_residual\"].quantile(0.05)\n",
    "vmax_res = tract_residuals_filtered[\"mean_residual\"].quantile(0.95)\n",
    "\n",
    "# Ensure color scale is centered at 0 by making vmin and vmax symmetric\n",
    "max_abs = max(abs(vmin_res), abs(vmax_res))\n",
    "vmin_centered = -max_abs\n",
    "vmax_centered = max_abs\n",
    "\n",
    "# Create choropleth with diverging colormap centered at 0\n",
    "residual_map.plot(\n",
    "    column=\"mean_residual\",\n",
    "    ax=ax,\n",
    "    cmap=\"RdBu_r\",  # Red = positive (slower), Blue = negative (faster)\n",
    "    edgecolor=\"gray\",\n",
    "    linewidth=0.3,\n",
    "    legend=True,\n",
    "    vmin=vmin_centered,\n",
    "    vmax=vmax_centered,\n",
    "    legend_kwds={\n",
    "        \"label\": \"Residual Delay (Hours)\",\n",
    "        \"orientation\": \"horizontal\",\n",
    "        \"shrink\": 0.6,\n",
    "    },\n",
    ")\n",
    "\n",
    "ax.set_title(\n",
    "    \"Residual Service Delays by Census Tract\\n(After Controlling for Service Type, Demographics, and Request Volume)\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../figures/residual_delays_choropleth.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== MAP INTERPRETATION ===\")\n",
    "print(\n",
    "    f\"Color scale clipped at 5th percentile ({vmin_res:.2f} hrs) and 95th percentile ({vmax_res:.2f} hrs)\"\n",
    ")\n",
    "print(\n",
    "    f\"Symmetric range: {vmin_centered:.2f} to {vmax_centered:.2f} hours (centered at 0)\"\n",
    ")\n",
    "print(\"\\nRed areas = Positive residuals (systematically SLOWER than model predicts)\")\n",
    "print(\"Blue areas = Negative residuals (systematically FASTER than model predicts)\")\n",
    "print(\"White/light areas = Near-zero residuals (delays match model predictions)\")\n",
    "print(\"\\nThese residuals control for:\")\n",
    "print(\"  • Service type (categorical fixed effects)\")\n",
    "print(\"  • Median household income\")\n",
    "print(\"  • Percent renter-occupied housing\")\n",
    "print(\"  • Total request volume per tract\")\n",
    "print(f\"\\n✓ Figure saved to ../figures/residual_delays_choropleth.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01df509",
   "metadata": {},
   "source": [
    "### Robustness Check: Median Residuals with Spearman Correlations\n",
    "\n",
    "To ensure our findings are robust to outliers and non-linear relationships, we repeat the analysis using:\n",
    "\n",
    "- **Median residuals** instead of mean (more resistant to extreme values)\n",
    "- **Spearman rank correlations** instead of Pearson (captures monotonic relationships without assuming linearity)\n",
    "\n",
    "We then compare which tracts appear as underperforming under both approaches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57befb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Identify top 15 tracts by MEDIAN residual (instead of mean)\n",
    "# Sort by median residual to identify worst performers using a robust metric\n",
    "\n",
    "tract_residuals_sorted_median = tract_residuals_filtered.sort_values(\n",
    "    \"median_residual\", ascending=False\n",
    ")\n",
    "\n",
    "# Get top 15 by median residual\n",
    "top_15_worst_median = tract_residuals_sorted_median.head(15)\n",
    "\n",
    "print(\"=== TOP 15 TRACTS BY MEDIAN RESIDUAL (Robust to Outliers) ===\")\n",
    "print(\n",
    "    \"(Tracts with highest median unexplained delays, more resistant to extreme values)\\n\"\n",
    ")\n",
    "display(\n",
    "    top_15_worst_median[\n",
    "        [\n",
    "            \"GEOID\",\n",
    "            \"median_residual\",\n",
    "            \"mean_residual\",\n",
    "            \"n_requests\",\n",
    "            \"median_household_income\",\n",
    "            \"pct_renter_occupied\",\n",
    "            \"poverty_rate\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\n=== COMPARISON: MEAN vs MEDIAN RESIDUAL ===\")\n",
    "print(\n",
    "    f\"Median residual range for top 15: {top_15_worst_median['median_residual'].min():.2f} to {top_15_worst_median['median_residual'].max():.2f} hours\"\n",
    ")\n",
    "print(\n",
    "    f\"Mean residual range for top 15: {top_15_worst_median['mean_residual'].min():.2f} to {top_15_worst_median['mean_residual'].max():.2f} hours\"\n",
    ")\n",
    "print(\n",
    "    f\"\\nAverage income of top 15 (median method): ${top_15_worst_median['median_household_income'].mean():,.0f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Average renter share of top 15 (median method): {top_15_worst_median['pct_renter_occupied'].mean()*100:.1f}%\"\n",
    ")\n",
    "print(\n",
    "    f\"Average poverty rate of top 15 (median method): {top_15_worst_median['poverty_rate'].mean()*100:.1f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918423fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Overlap analysis - Which tracts appear in BOTH top 15 lists?\n",
    "# Compare mean-based and median-based identification of worst performers\n",
    "\n",
    "top_15_mean_geoids = set(top_15_worst[\"GEOID\"])\n",
    "top_15_median_geoids = set(top_15_worst_median[\"GEOID\"])\n",
    "\n",
    "# Find overlap\n",
    "overlap_geoids = top_15_mean_geoids.intersection(top_15_median_geoids)\n",
    "mean_only = top_15_mean_geoids - top_15_median_geoids\n",
    "median_only = top_15_median_geoids - top_15_mean_geoids\n",
    "\n",
    "print(\"=== TRACT OVERLAP ANALYSIS ===\\n\")\n",
    "print(f\"Tracts in BOTH top 15 lists: {len(overlap_geoids)}\")\n",
    "print(f\"Tracts only in mean top 15: {len(mean_only)}\")\n",
    "print(f\"Tracts only in median top 15: {len(median_only)}\")\n",
    "print(f\"\\nOverlap rate: {len(overlap_geoids)/15*100:.1f}%\")\n",
    "\n",
    "if len(overlap_geoids) > 0:\n",
    "    print(f\"\\n=== CONSISTENT UNDERPERFORMERS (in both lists) ===\")\n",
    "    overlap_df = tract_residuals_filtered[\n",
    "        tract_residuals_filtered[\"GEOID\"].isin(overlap_geoids)\n",
    "    ].sort_values(\"mean_residual\", ascending=False)\n",
    "    display(\n",
    "        overlap_df[\n",
    "            [\n",
    "                \"GEOID\",\n",
    "                \"mean_residual\",\n",
    "                \"median_residual\",\n",
    "                \"n_requests\",\n",
    "                \"median_household_income\",\n",
    "                \"pct_renter_occupied\",\n",
    "            ]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "if len(mean_only) > 0:\n",
    "    print(f\"\\n=== TRACTS ONLY IN MEAN TOP 15 (potentially outlier-driven) ===\")\n",
    "    mean_only_df = tract_residuals_filtered[\n",
    "        tract_residuals_filtered[\"GEOID\"].isin(mean_only)\n",
    "    ].sort_values(\"mean_residual\", ascending=False)\n",
    "    display(\n",
    "        mean_only_df[\n",
    "            [\n",
    "                \"GEOID\",\n",
    "                \"mean_residual\",\n",
    "                \"median_residual\",\n",
    "                \"n_requests\",\n",
    "            ]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "if len(median_only) > 0:\n",
    "    print(f\"\\n=== TRACTS ONLY IN MEDIAN TOP 15 (more consistent delays) ===\")\n",
    "    median_only_df = tract_residuals_filtered[\n",
    "        tract_residuals_filtered[\"GEOID\"].isin(median_only)\n",
    "    ].sort_values(\"median_residual\", ascending=False)\n",
    "    display(\n",
    "        median_only_df[\n",
    "            [\n",
    "                \"GEOID\",\n",
    "                \"mean_residual\",\n",
    "                \"median_residual\",\n",
    "                \"n_requests\",\n",
    "            ]\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fd11cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Re-run regression with Spearman correlations approach\n",
    "# Use rank-based method that doesn't assume linear relationships\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# For Spearman, we compute correlations at the tract level (not request level)\n",
    "# between median residuals and demographic variables\n",
    "\n",
    "print(\"=== SPEARMAN RANK CORRELATIONS: MEDIAN RESIDUAL vs. DEMOGRAPHICS ===\")\n",
    "print(\"(Non-parametric test for monotonic relationships)\\n\")\n",
    "\n",
    "# Variables to test\n",
    "demo_vars_to_test = [\n",
    "    \"median_household_income\",\n",
    "    \"pct_renter_occupied\",\n",
    "    \"poverty_rate\",\n",
    "    \"n_requests\",\n",
    "]\n",
    "\n",
    "spearman_results = []\n",
    "\n",
    "for var in demo_vars_to_test:\n",
    "    # Get clean data (both variables present)\n",
    "    clean_data = tract_residuals_filtered[[var, \"median_residual\"]].dropna()\n",
    "\n",
    "    if len(clean_data) >= 5:\n",
    "        rho, p_value = spearmanr(clean_data[var], clean_data[\"median_residual\"])\n",
    "\n",
    "        # Interpret significance\n",
    "        if p_value < 0.001:\n",
    "            sig = \"***\"\n",
    "        elif p_value < 0.01:\n",
    "            sig = \"**\"\n",
    "        elif p_value < 0.05:\n",
    "            sig = \"*\"\n",
    "        else:\n",
    "            sig = \"ns\"\n",
    "\n",
    "        spearman_results.append(\n",
    "            {\n",
    "                \"Variable\": var,\n",
    "                \"Spearman_rho\": rho,\n",
    "                \"p_value\": p_value,\n",
    "                \"significance\": sig,\n",
    "                \"n_tracts\": len(clean_data),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"{var:30s}: ρ = {rho:7.3f}, p = {p_value:.4f} {sig} (n={len(clean_data)})\"\n",
    "        )\n",
    "\n",
    "spearman_df = pd.DataFrame(spearman_results)\n",
    "\n",
    "print(\"\\n\\nSignificance levels: *** p<0.001, ** p<0.01, * p<0.05, ns = not significant\")\n",
    "print(\"\\n=== INTERPRETATION ===\")\n",
    "print(\n",
    "    \"Positive ρ: Higher values of demographic variable → higher median residuals (slower)\"\n",
    ")\n",
    "print(\n",
    "    \"Negative ρ: Higher values of demographic variable → lower median residuals (faster)\"\n",
    ")\n",
    "print(\"\\nSpearman correlation captures ANY monotonic relationship (not just linear)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72653b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Summary comparison table - Mean/Pearson vs Median/Spearman\n",
    "\n",
    "print(\"=== COMPREHENSIVE COMPARISON: MEAN/PEARSON vs MEDIAN/SPEARMAN ===\\n\")\n",
    "\n",
    "# Create comparison summary\n",
    "comparison_summary = {\n",
    "    \"Metric\": [\n",
    "        \"Aggregation Method\",\n",
    "        \"Correlation Type\",\n",
    "        \"Resistance to Outliers\",\n",
    "        \"Assumes Linearity\",\n",
    "        \"Top 15 Tracts Identified\",\n",
    "        \"Overlap with Other Method\",\n",
    "        \"Avg Income of Top 15\",\n",
    "        \"Avg Renter % of Top 15\",\n",
    "        \"Avg Poverty % of Top 15\",\n",
    "    ],\n",
    "    \"Mean + Pearson\\n(Original)\": [\n",
    "        \"Mean residual\",\n",
    "        \"Pearson (linear)\",\n",
    "        \"Low (sensitive)\",\n",
    "        \"Yes\",\n",
    "        f\"{len(top_15_mean_geoids)} tracts\",\n",
    "        f\"{len(overlap_geoids)} ({len(overlap_geoids)/15*100:.0f}%)\",\n",
    "        f\"${top_15_worst['median_household_income'].mean():,.0f}\",\n",
    "        f\"{top_15_worst['pct_renter_occupied'].mean()*100:.1f}%\",\n",
    "        f\"{top_15_worst['poverty_rate'].mean()*100:.1f}%\",\n",
    "    ],\n",
    "    \"Median + Spearman\\n(Robust)\": [\n",
    "        \"Median residual\",\n",
    "        \"Spearman (rank)\",\n",
    "        \"High (robust)\",\n",
    "        \"No (monotonic only)\",\n",
    "        f\"{len(top_15_median_geoids)} tracts\",\n",
    "        f\"{len(overlap_geoids)} ({len(overlap_geoids)/15*100:.0f}%)\",\n",
    "        f\"${top_15_worst_median['median_household_income'].mean():,.0f}\",\n",
    "        f\"{top_15_worst_median['pct_renter_occupied'].mean()*100:.1f}%\",\n",
    "        f\"{top_15_worst_median['poverty_rate'].mean()*100:.1f}%\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_summary)\n",
    "display(comparison_df)\n",
    "\n",
    "print(\"\\n=== KEY FINDINGS ===\\n\")\n",
    "\n",
    "if len(overlap_geoids) >= 10:\n",
    "    print(\n",
    "        f\"✓ STRONG AGREEMENT ({len(overlap_geoids)}/15 tracts overlap): Results are ROBUST\"\n",
    "    )\n",
    "    print(\"  → These tracts consistently show unexplained delays regardless of method\")\n",
    "    print(\"  → High confidence in policy intervention targeting\")\n",
    "elif len(overlap_geoids) >= 5:\n",
    "    print(\n",
    "        f\"◐ MODERATE AGREEMENT ({len(overlap_geoids)}/15 tracts overlap): Some sensitivity\"\n",
    "    )\n",
    "    print(\"  → Core set of underperforming tracts identified\")\n",
    "    print(\"  → Additional tracts may be influenced by outliers or non-linearity\")\n",
    "else:\n",
    "    print(\n",
    "        f\"✗ LOW AGREEMENT ({len(overlap_geoids)}/15 tracts overlap): Results are SENSITIVE\"\n",
    "    )\n",
    "    print(\"  → Findings vary significantly by method\")\n",
    "    print(\"  → Recommend using median/Spearman for more robust identification\")\n",
    "\n",
    "print(\n",
    "    f\"\\nDemographic comparison: {'Similar' if abs(top_15_worst['median_household_income'].mean() - top_15_worst_median['median_household_income'].mean()) < 10000 else 'Different'} income profiles\"\n",
    ")\n",
    "print(\n",
    "    f\"Renter share comparison: {'Similar' if abs(top_15_worst['pct_renter_occupied'].mean() - top_15_worst_median['pct_renter_occupied'].mean()) < 0.1 else 'Different'} housing tenure\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
